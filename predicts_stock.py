# -*- coding: utf-8 -*-
"""Predicts Stock.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14wNsMhlqnav620Jsmxn29Kq5-CJTXClT
"""

#Description: This program uses an artificial recurrent neural network called Long Short Term Memory (LSTM)
#             To predict the closing stock price of a corporation (Apple Inc.) using the past 60 day stock price.

#Import the libraries
import math
import pandas_datareader as web
import numpy as np
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
from keras.models import Sequential
from keras.layers import Dense, LSTM
from sklearn.svm import SVR
import matplotlib.pyplot as plt
plt.style.use('fivethirtyeight')

#Get the stock quote
df = web.DataReader('AAPL',data_source='yahoo', start='2012-01-01', end='2020-12-24')
#Show the data
df

#Get the number of rows and columns in the data set
df.shape

#visualise the closing price history
figsize=0,0
plt.figure(figsize=(16,8))
plt.title('Close Price History')
plt.plot(df['Close'])
plt.xlabel('Date', fontsize=18)
plt.ylabel('Close Price USD ($)', fontsize=18)
plt.show()

#Create a new dataframe with only the close column
data = df.filter(['Close'])
#Convert the dataframe to a numpy array
dataset = data.values
#Get the numbe of rows to train the model on
training_data_len = math.ceil(len(dataset) * 0.8)          
training_data_len

#Scale the data
scaler = MinMaxScaler(feature_range=(0,1))
#Compute the minimum and maximum values to be used for scaling and then transforms the data based on the values (0-1)
scaled_data = scaler.fit_transform(dataset)
scaled_data

#Create the training dataset
#Create the scaled training dataset
train_data = scaled_data[0:training_data_len, :]
#Split the data into x_train and y_train data sets
x_train = []
y_train = []

for i in range(60, len(train_data)):
  x_train.append(train_data[i-60:i,0])
  y_train.append(train_data[i, 0])
  if i <= 61:
    print(x_train)
    print(y_train)
    print()

#Convert the x_train and y_train datasets to numpy array
x_train, y_train = np.array(x_train), np.array(y_train)

#Reshape the data
x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))
x_train.shape

#Build the LSTM and rbf model
model = Sequential()
model.add(LSTM(50, return_sequences=True, input_shape=(x_train.shape[1],1)))
model.add(LSTM(50, return_sequences=False))
model.add(Dense(25))
model.add(Dense(1))

#Compile the model
model.compile(optimizer='adam', loss='mean_squared_error')

#Train the model
#fit another name for train
#batch_size is the total number of training examples present in a single batch
#epochs is the number of iterations when an entire dataset is passed forwards and backwards through a neural network
model.fit(x_train, y_train, batch_size=1, epochs=1)

#Create the testing dataset
#Create a new array containing scaled values from index 1748 to 2260
test_data = scaled_data[training_data_len - 60: , :]
#Create the data sets x_test and y_test
x_test = []
#y_test will be all of the values that we want our model to predict
y_test = dataset[training_data_len:, :]
for i in range(60, len(test_data)):
  x_test.append(test_data[i-60:i, 0])

#Convert the data to a numpy array
x_test = np.array(x_test)
#Converting to a numpy array so that we can use it in the LSTM model

#Reshape the data
x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1))
#                            No of Rows       No of columns   #Close price

#Get the model's predicted price values
predictions = model.predict(x_test)
#Unscale the values
#We want predictions to contain the same values as our y_test dataset
#We are getting these values based of the x_test dataset
predictions = scaler.inverse_transform(predictions)

#Evaluate our model
#Get the root mean squared error (RMSE)
#Lower the value the more accurate the predictions
rmse = np.sqrt( np.mean( predictions - y_test )**2 )
rmse

#Plot the data
train = data[:training_data_len]
valid = data[training_data_len:]
valid['Predictions'] = predictions
#Visualise the data
plt.figure(figsize=(16,8))
plt.title('Apple Stock vs Machine Learing Prediction')
plt.xlabel('Date', fontsize=18)
plt.ylabel('Close Price USD ($)', fontsize=18)
plt.plot(train['Close'])
plt.plot(valid[['Close', 'Predictions']])
plt.legend(['Train', 'Val', 'Predictions'], loc='lower right')
plt.show()

#Show the valid predicted prices
valid

#Predict the closing price for apple stock for date
#Get the quote
apple_quote = web.DataReader('AAPL', data_source='yahoo', start='2012-01-01', end='2020-12-24')
#Create a new dataframe
new_df = apple_quote.filter(['Close'])
#Get the last 60 day closing price values and convert the dataframe to an array
last_60_days = new_df[-60:].values
#Scale the data to be values betweeen 0 and 1
last_60_days_scaled = scaler.transform(last_60_days)
#Create an empty list
X_test = []
#Append the past 60 days to the X_test list
X_test.append(last_60_days_scaled)
#Convert the X_test data set to a numpy array
X_test = np.array(X_test)
#Reshape the data to be 3d
X_test - np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))
#Get the predicted scale price
pred_price = model.predict(X_test)
#undo the scaling
pred_price = scaler.inverse_transform(pred_price)
#This is what the model belives the predicted price will be for date
print(pred_price)

#Get the quote
apple_quote2 = web.DataReader('AAPL', data_source='yahoo', start='2020-12-24', end='2020-12-24')
print(apple_quote2['Close'])